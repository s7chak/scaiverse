<p><i>Past the hype, what it is and what it can do.</i></p>
<br />
<br/>
<!-- <div
  className="scpost-img"
  style="display: flex; align-items: center; justify-content: center"
>
  <img
    style="border-radius: 5px"
    src="https://github.com/s7chak/Blogage/blob/master/pix/scaiverse-blogs/ai-human.jpg?raw=true"
    alt="The next mind"
  />
</div> -->
<br/>
<h2 style="font-size: 24px;"><b>What is this thing?</b></h2>
<br />
<p>
  I am in no sense an expert on language models, coming from the land of software merely dipping my feet in the ocean of machine learning. But here is what I understood of the current state of this "artificial intelligence" kraken. There are impressive, meh and real ugly implicative parts to this creature, so I want to begin by defining what this is. Four ideas led to this and here they are:<br /><br />
1. Artificial Intelligence is this broad domain of producing intelligent behavior from inorganic, non-living material or machines or silicon chips. It says nothing about how the intelligence may be arrived at - it could be computation, machine learning, combination of sensors and algorithms, complex programming or any other way we devise to make machines sound intelligent, or at least feel we are talking to a human.<br /><br />
2. Machine learning is the most common methodology used to achieve this intelligence and it is a essentially an optimization problem. Funny way to oversimplify a whole field, you might think, but at the heart of all models and algorithms that qualify as machine learning, there is some optimization going on to minimize the error in predicting some numbers, or training. In this field, we feed our silicon buddies loads of data to understand patterns in it. When data in the form of numbers is not available, say text or images, those are coded as numbers regardless and fed to a model to learn patterns in the numbers representing other things to humans. And as the names suggest supervised learning happens with labeled or marked targets, and unsupervised is the algorithm let loose on these numbers to find patterns on its own.<br /><br />
3. This branch of computer science began its journey with its core in mathematics like linear regressions, matrix algebra, gradient descent or the idea of iteratively reducing error in estimation of a function. Many algorithms were born in this phase like decision tree regressors, boosted decision trees which took the average of many tree regressors, a whole array of nuanced classifiers like Support Vector machines, Naive-Bayes and others. These were explanable in the sense we could tell why a certain prediction was thrown out of a model, individual importances of each input feature could be extracted from these "shallow" algorithms. Then came the neuron and neural networks with their own round of enhancements to keep getting better at predicting numbers. Essentially a neuron is a single unit that is at its core a function estimator, they are combined in layers with weights on each neuron to approximate a complex function. It is because of the layers this is known as deep learning and with it we lost explanability. Most aspects of the prediction process in a large enough neural network is difficult to visualize or explain in terms of why a particular output was arrived at. <br /><br />
4. The transformer architecture with multiple neural networks was the next step. Research this area led to a encode-decoder system, one part that encodes information whether text, numbers or image pixel data, the other that decodes the numerified patterns back to us as human readable or enjoyable formats. This architecture was enhanced further with an attention mechanism where the model can pay special attention to parts of training data, which leads to efficiency in the training process - we could train models with more data and it understood semantics, the hidden meanings of words and formed a more nuanced capacity for language. In the image and video world, these advancements were translated by converting images to matrices, and a lot of linear algebra, convolutional neural networks and pre-trained residual learners later object detection, facial recognition was no big deal at all. In the natural language modeling world, words were tokenized and represented as vectors of numbers for computers to understand, and then similar to number modeling neural networks entered this sphere as well and revolutionized it. Next came transformers, an innovation in neural network architecture, which gave special attention to parts of the training text to extract meaning, thus understanding semantics. More enhancements to transformers, vectors represented as embeddings and the fusion of these with some general architecture patterns(for error reduction) like mixture-of-experts gave us ChatGPT. <br /><br /><br />
And the floodgates opened.<br /><br />

<div
  className="scpost-img"
  style="display: flex; flex-direction: column; align-items: center; justify-content: center"
>
<img
    style="border-radius: 8px; width: auto; padding: 5px;"
    src="https://github.com/s7chak/Blogage/blob/master/pix/scaiverse-blogs/100m.png?raw=true"
    alt="AI Hype"
  />
  <p>The rise of AI popularity</p>
  <img
    style="border-radius: 8px; width: auto; padding: 5px;"
    src="https://github.com/s7chak/Blogage/blob/master/pix/scaiverse-blogs/aipubs.png?raw=true"
    alt="AI Boom"
  />
  <p>The rise of AI in academia.</p>
  
</div>
<br />
From passing the Turing test to passing actual real-world exams, from average writers to fully generated personas, from scrappy web developers to digital artists, from musicians, therapists, lawyers, doctors to AI entrepreneurs, a whole universe of AI opened up from the hype of one startup's chatty bot. Most likely because it didn't feel like a non-human - there were sparks of humanity. There is immense potential in this general purpose technology, which it truly is and will impact most industries on the planet. But potential can be directed both positively and negatively with huge promise and concern at the same time. And now that so much attention, mine included, is at this one technology I want to ponder over the many heads of this creature - the fields impacted, some thoughts on where we might be going with this new, kinda smart friend of ours.<br />
</p>
<br/>
Till then as Charlie Munger put it, "old-fashioned intelligence works pretty well."
<br />
<div
  className="scpost-img"
  style="display: flex; align-items: center; justify-content: center"
>
<img
    style="border-radius: 8px; width: auto;"
    src="https://github.com/s7chak/Blogage/blob/master/pix/scaiverse-blogs/gptvbrain.jpeg?raw=true"
    alt="Brain vs AI"
  />
</div>
<br />
<br />
<p>
  <b>Reference</b>
  <ul>
      <li>
        <a href="https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence" target="_blank">AI Timeline</a>
      </li>
      <li>
        <a href="https://ourworldindata.org/brief-history-of-ai" target="_blank">AI History</a>
      </li>
      <li>
          <a href="https://hai.stanford.edu/news/ai-index-state-ai-13-charts" target="_blank">AI Index</a>
      </li>
      <li>
        <a href="https://www.youtube.com/watch?v=_6R7Ym6Vy_I&pp=ygUObGFwYXRhIHRhbGsgYWk%3D" target="_blank">Images: Talk on Gen AI</a>
      </li>
      <!-- <li>
        Images: Bing (Dall-E)
      </li> -->
  </ul>
</p>
<br />